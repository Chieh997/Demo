{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL(w4) DNN - Learning Rate Scheduling\n",
    "student ID: 7110018036\\\n",
    "name: Chieh-An, Chou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'Adamax',\n",
       " 'Ftrl',\n",
       " 'Nadam',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'SGD',\n",
       " 'deserialize',\n",
       " 'experimental',\n",
       " 'get',\n",
       " 'legacy',\n",
       " 'schedules',\n",
       " 'serialize']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.optimizers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Momentum Optimizer\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{v}^{t}&=\\beta\\mathbf{v}^{t-1}-\\eta\\nabla_\\theta J(\\theta^{t-1})\\\\\n",
    "\\theta^t &= \\theta^{t-1}+\\mathbf{v}^t\\\\\n",
    "&= (\\theta^{t-1}+\\beta\\mathbf{v}^{t-1})-\\eta\\nabla_\\theta J(\\theta^{t-1})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=.001, momentum=.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nesterov Accelerated Gradient (NAG)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{v}^{t}&=\\beta\\mathbf{v}^{t-1}-\\eta\\nabla_\\theta J(\\theta^{t-1}+\\beta\\mathbf{v}^{t-1})\\\\\n",
    "\\theta^t &= \\theta^{t-1}+\\mathbf{v}^t\\\\\n",
    "&= (\\theta^{t-1}+\\beta\\mathbf{v}^{t-1})-\\eta\\nabla_\\theta J(\\theta^{t-1}+\\beta\\mathbf{v}^{t-1})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=.001, momentum=.9, nesterov=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adaptive Gradient (AdaGrad)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{s}^{t}&=\\mathbf{s}^{t-1}-\\nabla_\\theta J(\\theta^{t-1})\\otimes\\nabla_\\theta J(\\theta^{t-1})\\\\\n",
    "\\theta^t &= \\theta^{t-1}-\\eta\\nabla_\\theta J(\\theta^{t-1})\\oslash\\sqrt{\\mathbf{s}^{t}+\\epsilon}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adagrad(learning_rate=.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Root Mean Square Propagation (RMSProp)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{s}^{t}&=\\beta\\mathbf{s}^{t-1}-(1-\\beta) \\nabla_\\theta J(\\theta^{t-1})\\otimes\\nabla_\\theta J(\\theta^{t-1})\\\\\n",
    "\\theta^t &= \\theta^{t-1}-\\eta\\nabla_\\theta J(\\theta^{t-1})\\oslash\\sqrt{\\mathbf{s}^{t}+\\epsilon}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(learning_rate=.001, rho=.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adaptive Moment Estimation (Adam)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta^t &= \\theta^{t-1}-\\eta\\hat{\\mathbf{v}}^{t}\\oslash\\sqrt{\\hat{\\mathbf{s}}^{t}+\\epsilon}\\\\\n",
    "\\mathbf{v}^{t}&=\\beta_1\\mathbf{v}^{t-1}-(1-\\beta_1)\\nabla_\\theta J(\\theta^{t-1}),\\;&\n",
    "\\hat{\\mathbf{v}}^{t}= \\dfrac{\\mathbf{v}^{t}}{1-(\\beta_1)^t} \\\\\n",
    "\\mathbf{s}^{t}&=\\beta_2\\mathbf{s}^{t-1}-(1-\\beta_2) \\nabla_\\theta J(\\theta^{t-1})\\otimes\\nabla_\\theta J(\\theta^{t-1}),\\;&\n",
    "\\hat{\\mathbf{s}}^{t}= \\dfrac{\\mathbf{s}^{t}}{1-(\\beta_2)^t}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=.001, beta_1=.9, beta_2=.999)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. AdaMax\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{v}^{t}&=\\beta_1\\mathbf{v}^{t-1}-(1-\\beta_1)\\nabla_\\theta J(\\theta^{t-1}),\\;&\\hat{\\mathbf{v}}^{t}&= \\dfrac{\\mathbf{v}^{t}}{1-(\\beta_1)^t}\\\\\n",
    "\\mathbf{s}^{t}&=\\max(\\beta_2\\mathbf{s}^{t-1},\\|\\nabla_\\theta J(\\theta^{t-1})\\|)\\\\\n",
    "\\theta^t &= \\theta^{t-1}-\\eta\\hat{\\mathbf{v}}^{t}\\oslash(\\mathbf{s}^{t}+\\epsilon)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adamax(learning_rate=.001, beta_1=.9, beta_2=.999)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Nadam\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{g}}^t&=\\dfrac{\\mathbf{g}^t}{1-\\Pi_{i=1}^t\\mu_i},\\;&\\mathbf{g}^t&=\\nabla_\\theta J(\\theta^{t-1})\\\\\n",
    "\\mathbf{v}^{t}&=\\mu_t\\mathbf{v}^{t-1}-(1-\\mu_t)\\mathbf{g}^t,\\;&\n",
    "\\hat{\\mathbf{v}}^{t}&= \\dfrac{\\mathbf{v}^{t}}{1-\\Pi_{i=1}^{t+1}\\mu_i}\\\\\n",
    "\\bar{\\mathbf{v}}^t&=(1-\\mu_t)\\hat{\\mathbf{g}}^t+\\mu_{t+1}\\hat{\\mathbf{v}}^{t}\\\\\n",
    "\\mathbf{s}^{t}&=\\beta\\mathbf{s}^{t-1}-(1-\\beta) \\mathbf{g}^t\\otimes\\mathbf{g}^t,\\;&\n",
    "\\hat{\\mathbf{s}}^{t}&= \\dfrac{\\mathbf{s}^{t}}{1-(\\beta)^t}\\\\\n",
    "\\theta^t &= \\theta^{t-1}-\\eta\\bar{\\mathbf{v}}^{t}\\oslash\\sqrt{\\hat{\\mathbf{s}}^{t}+\\epsilon}\n",
    "\\end{align*}\n",
    "$$\n",
    "+ `beta_1`: initial value of $\\mu_t$\n",
    "+ `beta_2`: $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Nadam(learning_rate=.001, beta_1=.9, beta_2=.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train_set, y_train_set), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_set, y_train_set, random_state = 1)\n",
    "\n",
    "# Preprocessing\n",
    "pixel_means = x_train.mean(axis=0, keepdims=True) # (N, 28,28) -> (1,28,28)\n",
    "pixel_stds = x_train.std(axis=0, keepdims=True)\n",
    "x_train_scaled = (x_train-pixel_means)/pixel_stds\n",
    "x_valid_scaled = (x_valid-pixel_means)/pixel_stds\n",
    "x_test_scaled = (x_test-pixel_means)/pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear and setting random seed\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(units=300, activation='selu', kernel_initializer='lecun_normal'), \n",
    "    keras.layers.Dense(units=100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(units=10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose learning rate scheduling method: Expnential Decay\n",
    "opt = keras.optimizers.Nadam(learning_rate=.001, beta_1=.9, beta_2=.999)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.3143 - accuracy: 0.8845 - val_loss: 30.9328 - val_accuracy: 0.6213\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.2828 - accuracy: 0.8956 - val_loss: 28.1762 - val_accuracy: 0.6097\n"
     ]
    }
   ],
   "source": [
    "train = model.fit(x_train_scaled, y_train, epochs = 2,\n",
    "                  validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CosineDecay',\n",
       " 'CosineDecayRestarts',\n",
       " 'ExponentialDecay',\n",
       " 'InverseTimeDecay',\n",
       " 'LearningRateSchedule',\n",
       " 'PiecewiseConstantDecay',\n",
       " 'PolynomialDecay',\n",
       " 'deserialize',\n",
       " 'serialize']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.optimizers.schedules) if not name.startswith(\"_\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Polynomial Decay\n",
    "$$\n",
    "\\eta_t = (\\eta_0+\\eta_T)\\times(1+t/s)^c+\\eta_T\n",
    "$$\n",
    "+ $\\eta_0$: initial_learning_rate(.01)\n",
    "+ $\\eta_1$: end_learning_rate(.0001)\n",
    "+ $s$: decay steps, $s=\\begin{cases}s &\\text{, if cycle = False}\\\\s\\times\\text{ceil}(t/s)&\\text{, if cycle = True}\\end{cases}$\n",
    "+ $c$: power, default = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=.1,\n",
    "    decay_steps=1000,\n",
    "    end_learning_rate=1e-4,\n",
    "    power=.5,\n",
    "    cycle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exponential Decay\n",
    "$$\n",
    "\\eta_t = \\eta_0r^{t/s}\n",
    "$$\n",
    "+ $r$: decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_exp = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=.1,\n",
    "    decay_steps=1000000,\n",
    "    decay_rate=.96\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Piecewise Constant Decay\n",
    "$$\n",
    "\\eta = \\begin{cases}\n",
    "\\eta_0 &\\text{if }0\\le t\\le s_0\\\\ \n",
    "\\eta_1 &\\text{if }s_0\\le t\\le s_1\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "\\eta_T &\\text{if }s_{T-1}\\le t\\le t\n",
    "\\end{cases}\n",
    "$$\n",
    "+ boundaries: $[s_0,\\dots,s_{T-1}]$\n",
    "+ values: $[\\eta_0, \\dots,\\eta_{T-1},\\eta_T]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pcd = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[100000, 110000],\n",
    "    values=[1.0,0.5,0.1]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train_set, y_train_set), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_set, y_train_set, random_state = 1)\n",
    "\n",
    "# Preprocessing\n",
    "pixel_means = x_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = x_train.std(axis=0, keepdims=True)\n",
    "x_train_scaled = (x_train-pixel_means)/pixel_stds\n",
    "x_valid_scaled = (x_valid-pixel_means)/pixel_stds\n",
    "x_test_scaled = (x_test-pixel_means)/pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear and setting random seed\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(units=300, activation='selu', kernel_initializer='lecun_normal'), \n",
    "    keras.layers.Dense(units=100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(units=10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose learning rate scheduling method: Expnential Decay\n",
    "lr_exp = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=.1,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=.96\n",
    ")\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer=keras.optimizers.SGD(learning_rate=lr_exp),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.5091 - accuracy: 0.8214 - val_loss: 30.2518 - val_accuracy: 0.5715\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.3808 - accuracy: 0.8601 - val_loss: 19.1250 - val_accuracy: 0.5567\n"
     ]
    }
   ],
   "source": [
    "train = model.fit(x_train_scaled, y_train, epochs = 2,\n",
    "                  validation_data=(x_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
